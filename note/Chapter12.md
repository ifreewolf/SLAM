# 第 12 讲 建图

<div style="border: 1px solid black;">
<B>主要目标</B>

1. 理解单目SLAM中稠密深度估计的原理。
2. 通过实验了解单目稠密重建的过程。
3. 了解几种RGB-D重建中的地图形式。

</div>

本讲介绍建图部分的算法，在前端和后端中，重点关注同时估计相机运动轨迹与特征点空间位置的问题。然而，在实际使用SLAM时，除了对相机本体进行定位、导航、避障和交互，特征点地图显然不能满足所有的需求。所以，本讲我们将更详细地讨论各种形式的地图，并指出目前视觉SLAM地图中存在的缺陷。

## 12.1 概述

建图，本应该是SLAM的两大目标之一--因为SLAM被称为同时定位与建图。但是直到现在，我们讨论的都是定位问题，包括通过特征点的定位、直接法的定位，以及后端优化。那么，这是否暗示建图在SLAM里没那么重要，所以直到本讲才开始讨论呢？

答案是否定的。事实上，在经典的SLAM模型中，我们所谓的地图，即所有路标点的集合。一旦确定了路标点的位置，就可以说我们完成了建图。于是，前面说的视觉里程计也好，BA也好，事实上都建模了路标点的位置，并对它们进行了优化。从这个角度上说，我们已经探讨了建图问题。那么为何还要单独介绍建图呢？

这是因为人们对建图的需求不同。SLAM作为一种底层技术，往往是用来为上层应用提供信息的。如果上层是机器人，那么应用层的开发者可能希望使用SLAM做全局的定位，并且让机器人在地图中导航--例如扫地机需要完成扫地工作，希望计算一条能够覆盖整张地图的路径。或者，如果上层是一个增强现实设备，那么开发者可能希望将虚拟物体叠加在现实物体之中，特别地，还可能需要处理虚拟物体和真实物体的遮挡关系。

我们发现，应用层面对于“定位”的需求是相似的，希望SLAM提供相机或搭载相机的主体的空间位姿信息。而对于地图，则存在着许多不同的需求。从视觉SLAM的角度看，“建图”是服务于“定位”的；但是从应用层面看，“建图”明显带有许多其他的需求。关于地图的用处，我们大致归纳如下：

1. <B>定位</B>。定位是地图的一项基本功能。在前面的视觉里程计部分，我们讨论了如何利用局部地图实现定位。在回环检测部分，我们也看到，只要有全局的描述子信息，我们也能通过回环检测确定机器人的位置。我们还希望能够把地图保存下来，让机器人在下次开机后依然能在地图中定位，这样只需对地图进行一次建模，而不是每次启动机器人都重新做一次完整的SLAM。
2. <B>导航</B>。导航是指机器人能够在地图中进行路径规划，在任意两个地图点间寻找路径，然后控制自己运动到目标点的过程。在该过程中，我们至少需要知道<B>地图中哪些地方不可通过，而哪些地方是可以通过的</B>。这就超出了稀疏特征点地图的能力范围，必须有另外的地图形式。这至少是一种<B>稠密</B>的地图。
3. <B>避障</B>。避障也是机器人经常碰到的一个问题。它与导航类似，但更注重局部的、动态的障碍物的处理。同样，仅有特征点，无法判断某个特征点是否为障碍物，所以需要<B>稠密</B>地图。
4. <B>重建</B>。有时，我们希望利用SLAM获得周围环境的重建效果。这种地图主要用于向人展示，所以希望它看上去比较舒服、美观。或者，我们也可以把该地图用于通信，使其他人能够远程观看我们重建得到的三维物体或场景--例如三维的视频通话或者网上购物等。这种地图亦是<B>稠密</B>的，并且还对它的外观有一些要求。我们可能不满足于稠密点云重建，更希望能够构建带纹理的平面，就像电子游戏中的三维场景那样。
5. <B>>交互</B>。交互主要指人与地图之间的互动。例如，在增强现实中，我们会在房间里放置虚拟的物体，并与这些虚拟物体之间有一些互动--例如我们会点击墙面上放着的虚拟网页浏览器来观看视频，或者向墙面投掷物体，希望它们有(虚拟的)物体碰撞。另外，机器人应用中也会有与人、与地图之间的交互。例如，机器人可能会收到命令“取桌子上的报纸”，那么，除了有环境地图，机器人还需要知道哪一块地图是“桌子”，什么叫作“之上”，什么叫作“报纸”。这就需要机器人对地图有更高层面的认知--也称为语义地图。

图12-1形象地解释了上面讨论的各种地图类型与用途之间的关系。之前的讨论，基本集中于“稀疏路标地图”部分，还没有探讨稠密地图。所谓稠密地图是相对于稀疏地图而言的。稀疏地图只建模感兴趣的部分，也就是前面说了很久的特征点(路标点)。而稠密地图是指建模<B>所有</B>看到过的部分。对于同一张桌子，稀疏地图可能只建模了桌子的四个角，而稠密地图则会建模整个桌面。虽然从定位角度看，只有四个角的地图也可以用于对相机进行定位，但由于我们无法从四个角推断这几个点之间的空间结构，所以无法仅用四个角完成导航、避障等需要稠密地图才能完成的工作。

<div align=center>
    <img src="./image/各种地图的示意图.png" />
    <br />
    <P>图 12-1 各种地图的示意图</P>
</div>

从上面的讨论中可以看出，稠密地图占据着一个非常重要的位置。于是，剩下的问题是：通过视觉SLAM能建立稠密地图吗？如果能，怎么建？

---

## 12.2 单目稠密重建

### 12.2.1 立体视觉

相机，被认为是只有角度的传感器(Bearing only)，单幅图像中的像素，只能提供物体与相机成像平面的角度及物体采集到的亮度，而无法提供物体的距离(Range)。而在稠密重建中，我们需要知道每一个像素点(或大部分像素点)的距离，对此大致上有如下解决方案：

1. 使用单目相机，估计相机运动，并且三角化计算像素的距离。
2. 使用双目相机，利用左右目的视差计算像素的距离(多母原理相同)。
3. 使用RGB-D相机直接获得像素距离。

前两种方式称为立体视觉(Stereo Vision)，其中移动单目相机的又称为移动视角的立体视觉(Moving View Stereo, MVS)。相比于RGB-D直接测量的深度，使用单目和双目的方式对深度获取往往是“费力不讨好”的-计算量巨大，最后得到一些不怎么可靠的深度估计。当然，RGB-D也有一些量程、应用范围和光照的限制，不过相比于单目和双目的结果，使用RGB-D进行稠密重建往往是更常见的选择。而单目、双目的好处是，在目前RGB-D还无法被很好地应用的室外、大场景场合中，仍能通过立体视觉估计深度信息。

本节将实现一遍单目的稠密估计，体验为何说它是费力不讨好的。从最简单的情况讲起：在给定相机轨迹的基础上，如何根据一段时间的视频序列估计某幅图像的深度。这里不考虑SLAM，先考虑简单的建图问题。

假定有一段视频序列，我们得到了每一帧对应的轨迹(有可能由视觉里程计前端估计所得)。现在以第一幅图像为参考帧，计算参考帧中每个像素的深度(或者距离)。首先，请回忆在特征点部分，我们是如何完成该过程的：

1. 对图像提取特征，并根据描述子计算特征之间的匹配。换言之，通过特征，我们对某一个空间点进行了跟踪，知道了它在各个图像之间的位置。
2. 由于无法仅用一幅图像确定特征点的位置，所以必须通过不同视角下的观测估计它的深度，原理即前面讲过的三角测量。

在稠密深度图估计中，不同之处在于，无法把每个像素都当作特征点计算描述子。因此，稠密深度估计问题中，匹配就称为很重要的一环：如何确定第一幅图的某像素出现在其他图里的位置？这需要用到<B>极线搜索</B>和<B>块匹配技术</B>。当我们知道某个像素在各个图中的位置，就能像特征点那样，利用三角测量法确定它的深度。不过不同的是，在这里要使用很多次三角测量法让深度估计收敛，而不仅使用一次。我们希望深度估计能够随着测量的增加从一个非常不确定的量，逐渐收敛到一个稳定值。这就是<B>深度滤波器技术</B>。

### 12.2.2 极线搜索与块匹配

如下图12-2所示，左边的相机观测到了某个像素$p_1$，由于这是一个单目相机，无从知道它的深度，所以假设这个深度可能在某个区域之内，不妨说是某最小值到无穷远之间$(d_{\min}, +\infty)$。因此，该像素对应的空间点就分布在某条线段(本例中是射线)上。从另一个视角(右侧相机)看，这条线段的投影也形成图像平面上的一条线，我们知道这称为<B>极线</B>。当知道两部相机间的运动时，这条极线也是能够确定的。那么问题就是：极线上的哪一点是我们刚才看到的$p_1$点呢？

<div align=center>
    <img src="./image/极线搜索示意图.png" />
    <br />
    <P>图 12-2 极线搜索示意图</P>
</div>

在特征点法中，通过特征匹配找到了$p_2$的位置，然而，现在我们没有描述子，所以只能在极线上所有和$p_1$长得比较相似的点。再具体地说，我们可能沿着第二幅图像中的极线的某一头走到另一头，逐个比较每个像素与$p_1$的相似程度。从直接比较像素的角度来看，这种做法和直接法有异曲同工之妙。

在直接法的讨论中我们了解到，比较单个像素的亮度值并不一定可靠。一件很明显的事情就是：万一极线上有很多和$p_1$相似的点，怎么确定哪一个是真实的呢？这似乎回到了我们在回环检测中说到的问题：如何确定两幅图像(或两个点)的相似性？回环检测是通过词袋来解决的，但是这里由于没有特征，所以只好寻求另外的解决途径。

一种直观的想法是：既然单个像素的亮度没有区分性，是否可以比较像素块呢？我们在$p_1$周围取一个大小为$w\times w$的小块，然后在极线上也取很多同样大小的小块进行比较，就可以在一定程度上提高区分性。这就是所谓的<b>块匹配</b>。在这个过程中，只有假设在不同图像间整个小块的灰度值不变，这种比较才有意义。所以算法的假设，从像素的灰度不变性变成了图像块的灰度不变性--在一定程度上变得更强了。

现在我们取得了$p_1$周围的小块，并且在极线上也取了很多个小块。不妨把$p_1$周围的小块记成$A \in \mathbb{R}^{w\times w}$，把极线上的$n$个小块记成$B_i,i=1,...,n$。那么，如何计算小块与小块间的差异呢？有若干种不同的计算方法：

1. SAD(sum of Absolute Difference)。顾名思义，即取两个小块的差的绝对值之和：

$$
S(A,B)_{SAB} = \sum_{i,j}| A(i,j) - B(i,j)|. \tag{12.1}
$$

2. SSD。平方和(Sum of Squared Distance):

$$
S(A,B)_{SSD} = \sum_{i,j}(A(i,j) - B(i,j))^2. \tag{12.2}
$$

3. NNC(Normalized Cross Correlation, 归一化互相关)。这种方式比前面两种要复杂，它计算的是两个小块的相关性：

$$
S(A,B)_{NCC} = \frac{\sum_{i,j} A(i,j) B(i,j)}{\sqrt{\sum_{i,j}A(i,j)^2 \sum_{i,j}B(i,j)^2}}. \tag{12.3}
$$

由于这里用的是相关性，所以相关性接近0表示两幅图像不相似，接近1表示相似。前面两种距离则是反过来的，接近0表示相似，而大的数值表示不相似。

这些计算方式往往存在一个精度-效率之间的矛盾，精度好的方法往往需要复杂的计算，而简单的快速算法又往往效果不佳。这需要我们在实际工程中进行取舍。另外，除了这些简单版本，我们可以<B>先把每个小块的均值去掉</B>，称为去均值的SSD、去均值的NCC，等等。去掉均值之后，允许像“小块B比A整体上亮一些，但仍然很相似”这样的情况，因此比之前的更可靠。

现在，我们在极线上计算了$A$与每一个$B_i$的相似性度量。假设使用NCC，那么，将得到一个沿着极线的NCC分布。这个分布的形状取决于图像数据，如图12-3所示。

<div align=center>
    <img src="./image/匹配得分沿距离的分布.png" />
    <br />
    <P>图 12-3 匹配得分沿距离的分布</P>
</div>

在搜索距离较长的情况下，通常会得到一个非凸函数：这个分布存在许多峰值，然而真实的对应点必定只有一个。在这种情况下，我们会倾向于使用概率分布描述深度值，而非用某个单一的数值来描述深度。于是，我们的问题就转到了在不断地对不同图像进行极线搜索时，我们估计的深度分布将发生怎样的变化--这就是所谓的<B>深度滤波器</B>。

### 12.2.3 高斯分布的深度滤波器

对像素点深度的估计，本身也可建模为一个状态估计问题，于是就自然存在滤波器与非线性优化两种求解思路。虽然非线性优化效果较好，但是在SLAM这种实时性要求较强的场合，考虑到前端已经占据了不少的计算量，建图方面则通常采用计算量较少的滤波器方式。这也是本节讨论深度滤波器的目的。

对深度的分布假设存在若干种不同的做法。一方面，在比较简单的假设条件下，可以假设深度值服从高斯分布，得到一种类卡尔曼式的方法(但实际上只是归一化积)。先介绍并演示高斯分布假设下的深度滤波器。

设某个像素点的深度$d$服从：

$$
P(d) = N(\mu, \sigma^2). \tag{12.4}
$$

每当新的数据到来，我们都会观测到它的深度。同样，假设这次观测也是一个高斯分布：

$$
P(d_{obs}) = N(\mu_{obs}, \sigma_{obs}^2). \tag{12.5}
$$

于是，我们的问题是，如何使用观测的信息更新原先$d$的分布。这正是一个信息融合问题。两个高斯分布的乘积依然是一个高斯分布。设融合后的$d$分布为$N(\mu_{fuse}, \sigma_{fuse}^2)$，那么根据高斯分布的乘积，有

$$
\mu_{fuse} = \frac{\sigma_{obs}^2 \mu + \sigma^2 \mu_{obs}}{\sigma^2 + \sigma_{obs}^2}, \quad \sigma_{fuse}^2 = \frac{\sigma^2 \sigma_{obs}^2}{\sigma^2 + \sigma_{obs}^2}. \tag{12.6}
$$

由于我们仅有观测方程没有运动方程，所以这里深度仅用到了信息融合部分，而无须像完整的卡尔曼那样进行预测和更新(当然，可以把它看成“运动方程为深度值固定不动”的卡尔曼滤波器)。可以看到融合的方程确实浅显易懂，不过问题仍然存在：如何确定我们观测到深度的分布呢？即如何计算$\mu_{fuse},\sigma_{obs}$呢？

关于$\mu_{fuse},\sigma_{obs}$，也存在一些不同的处理方式。可以考虑几何不确定性和光度不确定性二者之和，或者仅考虑几何不确定性。我们暂时只考虑由几何关系带来的不确定性。现在，假设我们通过极线搜索和块匹配确定了参考帧某个像素在当前帧的投影位置。那么，这个位置对深度的不确定性有多大呢？

以图12-4为例。考虑某次极线搜索，我们找到了$p_1$对应的$p_2$点，从而观测到了$p_2$的深度值，认为$p_1$对应的三维点为$P$。从而，可记$O_1 P$为$p$,$O_1 O_2$为相机的平移$t$，$O_2 P$记为$a$。并且，把这个三角形的下面两个角记作$\alpha, \beta$。现在，考虑极线$l_2$上存在一个像素大小的误差，使得$\beta$角变成了$\beta'$，而$p_2$也变成了$p_2'$，并记上面那个角为$\gamma$。我们要问的是，这个像素的误差会导致$p'$与$p$产生多大的差距呢？

<div align=center>
    <img src="./image/不确定性分析.png" />
    <br />
    <P>图 12-4 不确定性分析</P>
</div>

这是一个典型的几何问题。来列写这些量之间的几何关系。显然有

$$
\begin{array}{l}
    \mathbf{a = p - t} \\
    \alpha = \arccos \left \langle \mathbf{p,t} \right \rangle \\
    \beta = \arccos \left \langle \mathbf{a, -t} \right \rangle.
\end{array} \tag{12.7}
$$

对$\mathbf{p_2}$扰动一个像素，将使得$\beta$产生一个变化量，成为$\beta'$。根据几何关系，有

$$
\begin{array}{l}
    \beta' = \arccos \left \langle \mathbf{O_2 p_2', -t} \right \rangle \\
    \gamma = \pi - \alpha - \beta'.
\end{array} \tag{12.8}
$$

于是，由正弦定理，$p'$的大小可以求得

$$
\lVert \mathbf{p'} \rVert = \lVert \mathbf{t} \rVert \frac{\sin \beta'}{\sin \gamma}. \tag{12.9}
$$

由此，我们确定了由单个像素的不确定引起的深度不确定性。如果认为极线搜索的块匹配仅有一个像素的误差，那么就可以设：

$$
\sigma_{obs} = \lVert \mathbf{p} \rVert - \lVert \mathbf{p'} \rVert. \tag{12.10}
$$

当然，如果极线搜索的不确定性大于一个像素，则我们可按照此推导放大这个不确定性。接下来的深度数据融合已经在前面介绍过了，在实际工程中，当不确定性小于一定阈值时，就可以认为深度数据已经收敛了。

综上所述，我们给出了估计稠密深度的一个完整的过程：

<div style="border: 1px solid black;">
<B>主要目标</B>

1. 假设所有像素的深度满足某个初始的高斯分布。
2. 当新数据产生时，通过极线搜索和块匹配确定投影点位置。
3. 根据几何关系计算三角化后的深度及不确定性。
4. 将当前观测融合进上一次的估计中。若收敛则停止计算，否则返回第2步。

</div>


这些步骤组成了一套可行的深度估计方式。请注意这里说的深度值是$O_1 P$的长度，它和我们在针孔相机模型里提到的“深度”有少许不同--针孔相机种的深度是指像素$z$值。

---

## 12.3 实践：单目稠密重建

### 12.3.1 实验分析与讨论

### 12.3.2 像素梯度的问题

对深度图像进行观察，会发现一件明显的事实。块匹配的正确与否依赖于图像块是否具有区分度。显然，如果图像块仅是一片黑或者一片白，缺少有效的信息，那么在NCC计算中就很有可能错误地将它与周围的某块像素匹配。演示程序中的打印机表面，它是均匀的白色，非常容易引起误匹配，因此打印机表面的深度信息多半是不正确的--演示程序的空间表面出现了明显不该有的条纹状深度估计，而根据我们直观的想象，打印机表面肯定是光滑的。

在进行块匹配(和NCC的计算)时，必须假设小块不变，然后将该小块与其他小块进行对比。这时，有<b>明显梯度</b>的小块将具有良好的区分度，不易引起误匹配。对于<b>梯度不明显的像素</b>，由于在块匹配时没有区分性，将难以有效地估计其深度。反之，像素梯度比较明显的地方，我们得到的深度信息也相对准确，例如桌面上的杂志、电话等具有明显<b>纹理</b>的物体。因此，演示程序反映了立体视觉中一个非常常见的问题：<b>对物体纹理的依赖性</b>。该问题在双目视觉中也极其常见，体现了立体视觉的重建质量十分依赖于环境纹理。从某种角度来说，该问题是<b>无法在现有的算法流程上加以改进并解决的</b>--如何我们依然只关心某个像素周围的领域(小块)的话。

以图12-7为例，看两种极端的例子：像素梯度平行于极线方向，以及垂直于极线方向。先来看垂直的情况，当我们沿着极线做块匹配时，会发现匹配程度都是一样的，因此得不到有效的匹配。反之，在平行的例子中，能够精确地确定匹配度最高点出现在何处。而实际中，梯度与极线的情况很可能介于二者之间：既不是完全垂直也不是完全平行。这时，我们说，当像素梯度与极线夹角较大时，极线匹配的不确定性大；而当夹角较小时，匹配的不确定性变小。

而在演示程序中，我们把这些情况都当成一个像素的误差，实际是不够精细的。考虑到极线与像素梯度的关系，应该使用更精确的不确定性模型。

<div align=center>
    <img src="./image/像素梯度与极线关系示意图.png" />
    <br />
    <P>图 12-7 像素梯度与极线关系示意图</P>
</div>


### 12.3.3 逆深度

从另一个角度看：把像素深度假设成高斯分布是否合适呢？在前面的内容中，经常用一个点的世界坐标$x,y,z$三个量来描述它，这是一种参数化形式。我们认为$x,y,z$三个量都是随机的，它们服从(三维的)高斯分布。但在本讲中，使用了图像坐标$u,v$和深度值$d$来描述某个空间点(即稠密建图)。我们认为$u,v$不动，而$d$服从（一维的）高斯分布，这是另一种参数化形式。那么，这两种参数化形式有什么不同吗？是否也能假设$u,v$服从高斯分布，从而形成另一种参数化形式呢？

不同的参数化形式，实际都描述了同一个量，也就是某个三维空间点。考虑到当在相机中看到某个点时，它的图像坐标$u,v$是比较确定的，而深度值$d$则是非常不确定的。此时，若用世界坐标$x,y,z$描述这个点，根据相机当前的位姿，$x,y,z$三个量之间可能存在明显的相关性。反映在协方差矩阵中，表现为非对角元素不为零。而如果用$u,v,d$参数化一个点，那么它的$u,v$和$d$至少是近似独立的，甚至我们还能认为$u,v$也是独立的--从而它的协方差矩阵近似为对角阵，更为简洁。

逆深度(Inverse depth)是近年来SLAM研究中出现的一种广泛使用的参数化技巧。在演示程序中，我们假设深度值满足高斯分布$d\sim N(\mu, \sigma^2)$。仔细想想，深度的正态分布确实存在一些问题：

1. 我们实际想表达的是：这个场景深度大概是$5\sim 10$米，可能有一些更远的点，但近处肯定不会小于相机焦距（或认为深度不会小于0）。这个分布并不是像高斯分布那样，形成一个对称的形状。它的尾部可能稍长，而负数区域则为零。
2. 在一些室外应用中，可能存在距离非常远，乃至无穷远处的点。我们的初始值中难以涵盖这些点，并且用高斯分布描述它们会有一些数值计算上的困难。 

于是，逆深度应用而生。人们在仿真中发现，假设深度的倒数，也就是<b>逆深度</b>，为高斯分布是比较有效的。随后，在实际应用中，逆深度也具有更好的数值稳定性，从而逐渐称为一种通用的技巧，存在于现有SLAM方案中的标准做法中。


### 12.3.4 图像间的变换

在块匹配之前，做一次图像到图像间的变换是一种常见的预处理方式。这是因为，我们假设了图像小块在相机运动时保持不变，而这个假设在相机平移时（示例数据集基本都是这样的例子）能够保持成立，但当相机发生明显的旋转时，就难以继续保持了。特别地，当相机绕光心旋转时，一块下黑上百的图像可能会变成一个上黑下白的图像块，导致相关性直接变成了负数（尽管仍然是同一块）。

为了防止这种情况的出现，通常需要在块匹配之前，把参考帧与当前帧之间的运动考虑进来。根据相机模型，参考帧上的一个像素$P_R$与真实的三维点世界坐标$P_W$有以下关系：

$$
d_R P_R = K(R_{RW}P_W + t_{RW}). \tag{12.12}
$$

类似的，对于当前帧，亦有$P_W$在它上边的投影，记作$P_C$:

$$
d_C P_C = K(R_{CW} P_W + t_{CW}). \tag{12.13}
$$

带入并消去$P_W$，即得两幅图像之间的像素关系：

$$
d_C P_C = d_R K R_{CW} R_{RW}^T K^{-1} P_R + K t_{CW} - K R_{CW} R_{RW}^T K t_{RW}. \tag{12.14}
$$

当知道$d_{R},P_R$时，可以计算出$P_C$的投影位置。此时，再给$P_R$两个分量各一个增量$du,dv$，就可以求得$P_C$的增量$d_{u_c}, d_{v_c}$。通过这种方式，算出在局部范围内参考帧和当前帧图像坐标变换的一个线性关系构成仿射变换：

$$
\begin{bmatrix}
    d_{u_c} \\ d_{v_c}
\end{bmatrix} = \begin{bmatrix}
    \frac{d_{u_c}}{du} & \frac{d_{u_c}}{dv} \\
    \frac{d_{v_c}}{du} & \frac{d_{v_c}}{dv}
\end{bmatrix} \begin{bmatrix}
    du \\ dv
\end{bmatrix} \tag{12.15}
$$

根据仿射变换矩阵，可以将当前帧（或参考帧）的像素进行变换，再进行块匹配，以期获得对旋转更好的效果。


### 12.3.5 并行化：效率的问题

在实验中可以看到，稠密深度图的估计非常费时，这是因为要估计的点从原先的数百个特征点一下子变成了几十万个像素点，即使现在主流的CPU也无法实时地计算那样庞大的数量。不过，该问题亦有另一个性质：这几十万个像素点的深度估计是彼此无关的！这使并行化游乐用武之地。

在示例程序中，我们在一个二重循环里遍历了所有像素，并逐个对它们进行极线搜索。当使用CPU时，这个过程是串行进行的：必须是上一个像素计算完毕后，再计算下一个像素。然而实际上，下一个像素完全没有必要等待上一个像素计算结束，因为它们之间并没有明显的联系，所以可以用多个线程，分别计算每个像素，然后将结果统一起来。理论上，如果我们有30万个线程，那么该问题的计算时间和计算一个像素是一样的。

GPU的并行计算架构非常适合这样的问题，因此，在单目和双目的稠密重建中，经常看到利用GPU进行并行加速的方式。


### 12.3.6 其他的改进

还能提出对本例程进行改进的方案，例如：

1. 现在各像素是独立计算的，可能存在这个像素深度很小，边上一个又很大的情况。我们可以假设深度图中相邻的深度变化不会太大，从而给深度估计加上了空间正则项。这种做法会使得到的深度图更加平滑。
2. 我们没有显式地处理外点（Outlier）的情况。事实上，由于遮挡、光照、运动模糊等各种因素的影响，不可能对每个像素都保持成功匹配。而演示程序的做法中，只要NCC大于一定值，就认为出现了成功的匹配，没有考虑到错误匹配的情况。
3. 处理错误匹配亦有若干种方式。例如均匀-高斯混合分布下的深度滤波器，显式地将内点与外点进行区分并进行概率建模，能够较好地处理外点数据。

从上面的讨论可以看出，存在许多可能的改进方案。如果细致地改进每一步的做法，最后是有希望得到一个良好的稠密建图的方案的。然而，有一些问题<b>存在理论上的困难</b>，例如对环境纹理的依赖，又如像素梯度与极线方向的关联（以及平行的情况。这些问题<b>很难通过调整代码实现来解决</b>。所以，直到目前为止，虽然双目和移动单目相机能够建立稠密的地图，但是我们通常认为它们过于依赖环境纹理和光照，不够可靠。

---

## 12.4 RGB-D稠密建图

除了使用单目和双目相机进行稠密重建，在适用范围内，RGB-D相机是一种更好的选择。利用RGB-D进行稠密建图是相对容易的。不过，根据地图形式不同，也存在着若干种不同的主流建图方式。最直观、最简单的方法就是根据估算的相机位姿，将RGB-D数据转化为点云，然后进行拼接，最后得到一个由离散的点组成的点云地图(Point Cloud Map)。在此基础上，如果我们对外观有进一步的要求，希望估计物体的表面，则可以使用三角网格(Mesh)、面片(Surfel)进行建图。另外，如果希望知道地图的障碍物信息并在地图上导航，也可通过体素(Voxel)建立占据网格地图(Occupancy Map)。

### 12.4.1 实践：点云地图

### 12.4.2 从点云重建网格

### 12.4.3 八叉树地图

下面介绍一种在导航中比较常用的、本身有较好的压缩性能的地图形式：<b>八叉树地图</b>（Octomap）。在点云地图中，我们虽然有了三维结构，也进行了体素滤波以调整分辨率，但是点云有几个明显的缺陷。

- 点云地图通常规模很大，所以pcd文件也会很大。一幅640像素$\times$ 480像素的图像，会产生30万个空间点，需要大量的存储空间。即使经过一些滤波后，pcd文件也是很大的。而且讨厌之处在于，它的“大”并不是必需的。点云地图提供了很多不必要的细节。我们并不是特别关心地摊上的褶皱、阴暗处的影子这类东西，把它们放在地图里是在浪费空间。由于这些空间的占用，除非我们降低分辨率，否则在有限的内存中无法建模较大的环境，然而降低分辨率会导致地图质量下降。有没有什么方式对地图进行压缩，舍弃一些重复的信息呢？
- 点云地图无法处理这些运动物体。因为我们的做法里只有“添加点”，而没有“当点消失时把它移除”的做法。而在实际环境中，运动物体的普遍存在，使得点云地图变得不够实用。

我们知道，把三维空间建模为许多个小方块（或体素）是一种常见的做法。如果我们把一个小方块的每个面平均切成两片，那么这个小方块就会变成同样大小的八个小方块。这个步骤可以不断地重复，直到最后的方块达到建模的最高精度。在这个过程中，把“将一个小方块分成同样大小的八个”这件事，看成“从一个节点展开成8个子节点”，那么，整个从最大空间细分到最小空间的过程，就是一颗八叉树。

如图12-11所示，左侧显示了一个大立方体不断地均匀分成八块，直到变成最小的方块为止。于是，整个大方块可以看作根节点，而最小的块可以看做“叶子节点”。于是，在八叉树中，当我们由下一层节点往上走一层时，地图的体积就能扩大为原来的八倍。我们不妨做一点简单的计算：如果叶子节点的方块大小为$1 \mathbf{cm}^3$，那么当我们限制八叉树为10层时，总共能建模的体积大约为$8^{10}\mathbf{cm}^3 = 1,073\mathbf{m}^3$，这足够建模一间屋子了。由于体积与深度呈指数关系，所以当我们用更大的深度时，建模的体积会增长得非常快。

<div align=center>
    <img src="./image/八叉树示意图.png" />
    <br />
    <P>图 12-11 八叉树示意图</P>
</div>

在点云的体素滤波器中，我们也限制了一个体素中只有一个点，为何说点云占空间，而八叉树比较节省空间？这是因为，在节点存储它是否被占据的信息。当某个方块的所有子节点都被占据或不被占据时，就<b>没必要展开这个节点</b>。例如，一开始地图为空白时，我们只需要一个根节点，不需要完整的树。当向地图中添加信息时，由于实际的物体经常连在一起，空白的地方也会常常连在一起，所以大多数八叉树节点无需展开到叶子层面。所以说，八叉树比点云节省大量的存储空间。

前面说八叉树的节点存储了它是否被占据的信息。从点云层面来讲，自然可以用0表示空白，1表示被占据。这种$0-1$的表示可以用一个比特来存储，节省空间，不过显得有些过于简单了。由于噪声的影响，可能会看到某个点一会儿为0，一会儿为1；或者多数时刻为0，少数时刻为1；或者除了“是”“否”两种情况，还有一个“未知”的状态。能否更精确地描述这件事呢？我们会选择用<b>概率</b>形式表达某节点是否被占据的事情。例如，用一个浮点数$x \in [0,1]$来表达。这个$x$一开始取0.5。如果不断地观测到它被占据，那么让这个值不断增加；反之，如果不断观测到它是空白的，那就让它不断减小即可。

通过这种方式，可以动态地建模地图中的障碍物信息。不过，现在的方式有一点小问题：如果让$x$不断增加或减小，它可能跑到$[0,1]$区间之外，带来处理上的不便。所以我们不是直接用概率来描述某节点被占据，而是用概率对数值（Log-odds）来描述。设$y\in \mathbb{R}$为概率对数值，$x$为$0\sim 1$的概率，那么它们之间的变换由logit变换描述：

$$
y = logit(x) = \log\left( \frac{x}{1-x} \right). \tag{12.16}
$$

其反变换为

$$
x = logit^{-1}(y) = \frac{\exp(y)}{\exp(y) + 1}. \tag{12.17}
$$

可以看到，当$y$从$-\infty$变到$\infty$时，$x$相应地从0变到了1。而当$y$取0时，$x$取0.5。因此，不妨存储$y$来表达节点是否被占据。当不断观测到“占据”时，让$y$增加一个值；否则就让$y$减小一个值。当查询概率时，再用逆logit变换，将$y$转换至概率即可。用数学形式来说，设某节点为$n$，观测数据为$z$。那么从开始到$t$时刻某节点的概率对数值为$L(n|z_{1:t})$，$t+1$时刻为

$$
L(n|z_{1:t+1}) = L(n|z_{1:t-1}) + L(n|z_t). \tag{12.18}
$$

如果写成概率形式而不是概率对数形式，就会有一点复杂：

$$
P(n|z_{1:T}) = \left[ 1 + \frac{1 - P(n|z_T)}{P(n|z_T)} \frac{1 - P(n|z_{1:T-1})}{P(n|z_{1:T-1})}  \frac{P(n)}{1 - P(n)} \right]^{-1}. \tag{12.19}
$$

有了对数概率，就可以根据RGB-D数据更新整个八叉树地图了。假设在RGB-D图像中观测到某个像素带有深度$d$，就说明：<b>在深度值对应的空间点上观察到了一个占据数据，并且，从相机光心出发到这个点的线段上应该是没有物体的</b>（否则会被遮挡）。利用这个信息，可以很好地对八叉树地图进行更新，并且能处理运动的结构。


### 12.4.4 实践：八叉树地图

---

## 12.5 *TSDF地图和Fusion系列

---

## 12.6 小结



